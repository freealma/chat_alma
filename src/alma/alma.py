"""
---
name: alma.py  
title: "Alma - CLI Principal con Optimizaci√≥n Integrada"
version: 0.0.6
changelog: "Fix: LLM real en lugar de respuesta simulada, mejor manejo de comandos"
path: src/alma/alma.py
description: "CLI principal con llamadas reales a DeepSeek"
functions: [main, chat_mode, optimize_mode, metrics_mode, call_deepseek_natural]
functions_descriptions:
  - main: "Funci√≥n principal con argparse"
  - chat_mode: "Modo chat interactivo con LLM real"
  - optimize_mode: "Modo optimizaci√≥n"
  - metrics_mode: "Mostrar m√©tricas del sistema"
  - call_deepseek_natural: "Llamada real a la API de DeepSeek"
tags: [alma, cli, deepseek, memory, optimization]
---
"""
#!/usr/bin/env python3
import os
import sys
import argparse
import requests
from .memory import MemoryManager

# Imports compatibles con LangChain
try:
    from langchain.agents import initialize_agent, AgentType
    from langchain.llms import OpenAI
    from langchain.memory import ConversationBufferMemory
    from langchain.tools import Tool
    
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("‚ö†Ô∏è  LangChain no disponible, usando modo est√°ndar")

def get_api_key():
    """Obtiene API key de variables de entorno"""
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("‚ùå DEEPSEEK_API_KEY no encontrada")
        print("   Aseg√∫rate de tener un archivo .env con DEEPSEEK_API_KEY=tu_key")
        sys.exit(1)
    return api_key

def call_deepseek_natural(api_key: str, message: str, context_summary: str) -> str:
    """
    Llama a la API REAL de DeepSeek para generar respuestas naturales
    """
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # System prompt mejorado para respuestas naturales
    system_msg = f"""Eres Alma, un asistente especializado en hacking y programaci√≥n.

Contexto disponible:
{context_summary}

**Instrucciones importantes:**
- Responde de forma NATURAL y conversacional
- Integra el conocimiento del contexto de forma org√°nica, NO lo listes
- Evita frases como "bas√°ndome en mis memorias" o "seg√∫n mi conocimiento"
- Enf√≥cate en dar la respuesta √∫til directamente
- S√© conciso pero completo
- Mant√©n un tono t√©cnico pero accesible

Si la informaci√≥n del contexto es relevante, √∫sala sin mencionar expl√≠citamente de d√≥nde viene."""
    
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": message}
        ],
        "temperature": 0.8,           # M√°s creatividad
        "max_tokens": 800,            # Respuestas adecuadas
        "frequency_penalty": 0.5,     # Evita repeticiones
        "presence_penalty": 0.3,      # Introduce variedad
        "stream": False
    }
    
    try:
        print("üîÑ Consultando a DeepSeek...")
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            json=data, 
            headers=headers, 
            timeout=30
        )
        response.raise_for_status()
        result = response.json()['choices'][0]['message']['content']
        return result
    except Exception as e:
        return f"‚ùå Error al contactar DeepSeek: {e}"

def setup_langchain_agent(api_key: str, memory_manager: MemoryManager):
    """Configura el agente de LangChain con herramientas reales"""
    if not LANGCHAIN_AVAILABLE:
        return None
    
    try:
        # Wrapper personalizado para DeepSeek
        class DeepSeekLLM:
            def __init__(self, api_key, temperature=0.7, max_tokens=800):
                self.api_key = api_key
                self.temperature = temperature
                self.max_tokens = max_tokens
            
            def __call__(self, prompt):
                return call_deepseek_natural(self.api_key, prompt, "")
        
        llm = DeepSeekLLM(api_key=api_key)
        memory = ConversationBufferMemory(memory_key="chat_history")
        
        # Herramientas reales
        def search_memories_tool(query: str) -> str:
            """Herramienta real para buscar en memorias"""
            try:
                context_summary = memory_manager.get_context_summary(query)
                return context_summary if context_summary else "No hay contexto relevante para esta consulta."
            except Exception as e:
                return f"Error buscando memorias: {e}"
        
        def add_memory_tool(content: str) -> str:
            """Herramienta real para agregar memorias"""
            try:
                success = memory_manager.add_memory(content)
                return "‚úÖ Memoria guardada exitosamente" if success else "‚ùå Error guardando memoria"
            except Exception as e:
                return f"Error: {e}"
        
        def list_memories_tool(query: str = "") -> str:
            """Herramienta real para listar memorias"""
            try:
                memories = memory_manager.search_memories_simple("", limit=8)
                if not memories:
                    return "No hay memorias guardadas actualmente."
                
                result = "üìö Memorias recientes:\n"
                for i, mem in enumerate(memories, 1):
                    preview = mem['content'][:60].replace('\n', ' ').strip()
                    result += f"{i}. {preview}...\n"
                return result
            except Exception as e:
                return f"Error: {e}"
        
        tools = [
            Tool(
                name="BuscarContexto",
                func=search_memories_tool,
                description="Buscar conocimiento relevante en las memorias para la consulta actual"
            ),
            Tool(
                name="GuardarConocimiento", 
                func=add_memory_tool,
                description="Guardar informaci√≥n importante en las memorias para uso futuro"
            ),
            Tool(
                name="VerMemorias",
                func=list_memories_tool,
                description="Mostrar un resumen de las memorias recientes guardadas"
            )
        ]
        
        # Crear agente real
        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
            memory=memory,
            verbose=False,
            handle_parsing_errors=True
        )
        
        return agent
        
    except Exception as e:
        print(f"‚ùå Error configurando LangChain: {e}")
        return None

def chat_mode(memory_manager, use_langchain=True):
    """Modo chat interactivo CON LLM REAL"""
    api_key = get_api_key()
    agent = None
    
    if LANGCHAIN_AVAILABLE and use_langchain:
        print("üîß Configurando agente LangChain...")
        agent = setup_langchain_agent(api_key, memory_manager)
        mode = "LangChain"
    else:
        mode = "Est√°ndar"
    
    print(f"ü§ñ Alma CLI v0.0.6 - Chat con Memoria ({mode})")
    print("üí¨ Escribe tu mensaje o /help para comandos")
    print()
    
    while True:
        try:
            user_input = input("üßë T√∫: ").strip()
            
            if user_input.lower() in ['exit', 'quit', 'salir']:
                print("üëã ¬°Hasta luego!")
                break
            
            if user_input == '/help':
                print("\nüìù Comandos disponibles:")
                print("  /add <texto>      - Guardar memoria")
                print("  /memories         - Listar memorias recientes")
                print("  /metrics          - Mostrar m√©tricas del sistema")
                print("  /optimize         - Ejecutar optimizaci√≥n manual")
                print("  /exit             - Salir")
                print()
                continue
            
            if user_input.startswith('/add '):
                content = user_input[5:].strip()
                if content:
                    success = memory_manager.add_memory(content)
                    print("‚úÖ Memoria guardada" if success else "‚ùå Error guardando memoria")
                continue
            
            if user_input == '/memories':
                memories = memory_manager.search_memories_simple("", limit=8)
                print("\nüìö Memorias recientes:")
                for i, mem in enumerate(memories, 1):
                    preview = mem['content'][:70].replace('\n', ' ').strip()
                    print(f"  {i}. {preview}...")
                print()
                continue
            
            if user_input == '/metrics':
                metrics = memory_manager.get_learning_metrics()
                print("\nüìä M√©tricas del sistema:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")
                print()
                continue
            
            if user_input == '/optimize':
                print("üîß Ejecutando optimizaci√≥n manual...")
                memory_manager.run_post_chat_optimization({'manual': True})
                continue
            
            if not user_input:
                continue
            
            # Procesar mensaje normal CON LLM REAL
            _process_message(api_key, memory_manager, user_input, agent)
            
        except KeyboardInterrupt:
            print("\nüëã ¬°Hasta luego!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")

def _process_message(api_key: str, memory_manager: MemoryManager, user_input: str, agent=None):
    """Procesar un mensaje individual CON LLM REAL"""
    print("üí≠ Buscando contexto relevante...")
    
    # Obtener contexto real
    context_summary = memory_manager.get_context_summary(user_input)
    
    if context_summary:
        print(f"   üìö {context_summary}")
    else:
        print("   üí° Sin contexto espec√≠fico disponible")
    
    # Generar respuesta REAL con DeepSeek
    try:
        if agent and LANGCHAIN_AVAILABLE:
            print("ü§ñ Procesando con LangChain...")
            response = agent.run(input=user_input)
        else:
            print("ü§ñ Generando respuesta...")
            response = call_deepseek_natural(api_key, user_input, context_summary)
        
        print(f"ü§ñ Alma: {response}\n")
        
        # Crear memoria autom√°ticamente si es valioso
        memory_manager.create_memory_from_conversation(user_input, response)
        
    except Exception as e:
        print(f"‚ùå Error generando respuesta: {e}")

def optimize_mode(memory_manager, batch_size=10):
    """Modo optimizaci√≥n manual"""
    print("üîß Ejecutando optimizaci√≥n manual...")
    
    try:
        from .memory_optimizer import MemoryOptimizer
        optimizer = MemoryOptimizer(memory_manager.db_path, memory_manager.api_key)
        
        print(f"üì¶ Procesando lotes de {batch_size} memorias...")
        results = optimizer.full_optimization(batch_size=batch_size)
        
        print("\n‚úÖ Optimizaci√≥n manual completada")
        return results
        
    except ImportError as e:
        print(f"‚ùå Optimizador no disponible: {e}")
        return None

def metrics_mode(memory_manager):
    """Mostrar m√©tricas detalladas"""
    metrics = memory_manager.get_learning_metrics()
    
    print("üìä M√âTRICAS DETALLADAS DEL SISTEMA")
    print("=" * 40)
    
    print("ü§ñ Sistema de Aprendizaje:")
    for key, value in metrics.items():
        if key not in ['conversation_patterns', 'last_adaptation']:
            print(f"  {key}: {value}")
    
    print("\nüíæ Base de Datos:")
    memories = memory_manager.search_memories_simple("", limit=1000)
    total_memories = len(memories)
    memory_types = {}
    
    for memory in memories:
        mem_type = memory.get('memory_type', 'unknown')
        memory_types[mem_type] = memory_types.get(mem_type, 0) + 1
    
    print(f"  Total memorias: {total_memories}")
    for mem_type, count in memory_types.items():
        print(f"  - {mem_type}: {count}")
    
    print(f"\nüîÑ √öltima optimizaci√≥n: {metrics.get('last_optimization', 'N/A')}")

def main():
    """Funci√≥n principal con argparse"""
    parser = argparse.ArgumentParser(
        description='Alma - Asistente con Memoria y Optimizaci√≥n Autom√°tica',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Modos de uso:
  alma                          # Modo chat interactivo (default)
  alma --optimize               # Solo optimizaci√≥n
  alma --metrics               # Solo m√©tricas
  alma --batch 15              # Optimizaci√≥n con lote espec√≠fico

Ejemplos:
  alma --optimize --batch 20    # Optimizar 20 memorias
  alma --metrics               # Ver estad√≠sticas
  alma --no-langchain          # Chat sin LangChain
        '''
    )
    
    parser.add_argument('--optimize', action='store_true', help='Modo optimizaci√≥n')
    parser.add_argument('--metrics', action='store_true', help='Mostrar m√©tricas')
    parser.add_argument('--batch', type=int, default=10, help='Tama√±o de lote para optimizaci√≥n')
    parser.add_argument('--no-langchain', action='store_true', help='Deshabilitar LangChain')
    parser.add_argument('--db-path', default='/alma/db/alma.db', help='Ruta de la base de datos')
    
    args = parser.parse_args()
    
    # Configurar memory manager
    api_key = get_api_key()
    memory_manager = MemoryManager(db_path=args.db_path, api_key=api_key)
    
    if args.metrics:
        metrics_mode(memory_manager)
    elif args.optimize:
        optimize_mode(memory_manager, batch_size=args.batch)
    else:
        # Modo chat por defecto
        chat_mode(memory_manager, use_langchain=not args.no_langchain)

if __name__ == "__main__":
    main()